from pylearn2.costs.cost import DefaultDataSpecsMixin, Cost
import theano.tensor as T
from pylearn2.expr.nnet import kl

class CascadeCost(DefaultDataSpecsMixin, Cost):
    """
    Represents an objective function to be minimized by some
    `TrainingAlgorithm`.
    """

    # If True, the data argument to expr and get_gradients must be a
    # (X, Y) pair, and Y cannot be None.
    supervised = True

    def __init__(self, thresholds):

      self.thresholds = thresholds
      self.thresholds.insert(0,0)


    def kl(self, Y, Y_hat, batch_axis):
        """
        Computes the KL divergence.


        Parameters
        ----------
        Y : Variable
            targets for the sigmoid outputs. Currently Y must be purely binary.
            If it's not, you'll still get the right gradient, but the
            value in the monitoring channel will be wrong.
        Y_hat : Variable
            predictions made by the sigmoid layer. Y_hat must be generated by
            fprop, i.e., it must be a symbolic sigmoid.

        Returns
        -------
        ave : Variable
            average kl divergence between Y and Y_hat.

        Notes
        -----
        Warning: This function expects a sigmoid nonlinearity in the
        output layer and it uses kl function under pylearn2/expr/nnet/.
        Returns a batch (vector) of mean across units of KL
        divergence for each example,
        KL(P || Q) where P is defined by Y and Q is defined by Y_hat:

        p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
        For binary p, some terms drop out:
        - p log q - (1-p) log (1-q)
        - p log sigmoid(z) - (1-p) log sigmoid(-z)
        p softplus(-z) + (1-p) softplus(z)
        """
        div = kl(Y=Y, Y_hat=Y_hat, batch_axis=batch_axis)
        return div


    def expr(self, model, data, ** kwargs):
        """
        Returns a theano expression for the cost function.

        Parameters
        ----------
        model : a pylearn2 Model instance
        data : a batch in cost.get_data_specs() form
        kwargs : dict
            Optional extra arguments. Not used by the base class.
        """

        space, sources = self.get_data_specs(model)
        space.validate(data)
        (x, targets) = data


        ### Cascade fprop
        output_list = list()
        thresh_list = list()
        for i in xrange(1, len(self.models)):
	  output_list.append(self.model[i].fprop(x(i)))
	  thresh_list.append(output_list[i] > self.threhsolds[i])
        out = output_list[-1]
        for i in xrange(len(output_list) - 2, 0, -1):
            out  = thresh_list[i] * out  + (1-thresh_list[i]) * output_list[i]
        P_y_given_x = out

	if isinstance(model[0].layers[-1], Sigmoid):
            total = self.kl(Y=P_y_given_x, Y_hat=target)
            rval = total.mean()
	else:
	  rval = -T.log(P_y_given_x)
	  rval = T.sum(rval, axis=1)

        return rval

	#cost_model_i = self.kl(Y=targets, Y_hat=output_model_i, batch_axis=axes.index('b'))
        #rlist.append(cost_model_i.mean())


        return tuple(rlist)


    def get_monitoring_channels(self, model, data, **kwargs):
        """
        .. todo::

            WRITEME

        .. todo::

            how do you do prereqs in this setup? (I think PL changed
            it, not sure if there still is a way in this context)

        Returns a dictionary mapping channel names to expressions for
        channel values.

        Parameters
        ----------
        model : Model
            the model to use to compute the monitoring channels
        data : batch
            (a member of self.get_data_specs()[0])
            symbolic expressions for the monitoring data
        kwargs : dict
            used so that custom algorithms can use extra variables
            for monitoring.

        Returns
        -------
        rval : dict
            Maps channels names to expressions for channel values.
        """

	rval = super(TeacherRegressionCost, self).get_monitoring_channels(model,data)

        value_cost_wrt_target = self.cost_wrt_target(model,data)
        if value_cost_wrt_target is not None:
	   name = 'cost_wrt_target'
	   rval[name] = self.wtarget*T.mean(value_cost_wrt_target)

        value_cost_wrt_teacher = self.cost_wrt_teacher(model,data)
        if value_cost_wrt_teacher is not None:
	   name = 'cost_wrt_teacher'
	   rval[name] = self.wteach*T.mean(value_cost_wrt_teacher)

	rval['wteach'] = self.wteach

        return rval






